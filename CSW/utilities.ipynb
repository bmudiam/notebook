{
 "metadata": {
  "name": "",
<<<<<<< HEAD
  "signature": "sha256:9444ef76d33ae49cee84a879d8d1634e24e166c835580446596a3d8adeac7077"
=======
  "signature": "sha256:142bedee5a6f285f9df0c4ba7a6f7761f374529348a5737c44f39c6d184ea759"
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
<<<<<<< HEAD
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some misc functions and constants used in IOOS_inundation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sos_name = 'water_surface_height_above_reference_datum'\n",
      "\n",
      "# Now we need to specify all the names we know for water level, names that\n",
      "# will get used in the CSW search, and also to find data in the datasets that\n",
      "# are returned.  This is ugly and fragile.  There hopefully will be a better\n",
      "# way in the future...\n",
      "\n",
      "std_name_list = ['water_surface_height_above_reference_datum',\n",
      "                 'sea_surface_height_above_geoid',\n",
      "                 'sea_surface_elevation',\n",
      "                 'sea_surface_height_above_reference_ellipsoid',\n",
      "                 'sea_surface_height_above_sea_level',\n",
      "                 'sea_surface_height',\n",
      "                 'water level']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard library:\n",
=======
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard Library.\n",
      "from lxml import etree\n",
      "from io import BytesIO\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "try:\n",
      "    from urllib.request import urlopen\n",
      "except ImportError:\n",
      "    from urllib import urlopen\n",
      "\n",
<<<<<<< HEAD
      "from lxml import etree\n",
      "from io import BytesIO\n",
      "\n",
      "# Scientific Stack.\n",
      "import iris\n",
      "import numpy as np\n",
      "import pandas as pd\n",
=======
      "# Scientific stack.\n",
      "import numpy as np\n",
      "from pandas import DataFrame, concat, read_csv\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "\n",
      "# Custom IOOS/ASA modules (available at PyPI).\n",
      "from owslib import fes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "def nearxy(x, y, xi, yi):\n",
      "    \"\"\"Find the indices x[i] of arrays (x,y) closest to the points (xi,yi).\"\"\"\n",
      "    ind = np.ones(len(xi), dtype=int)\n",
      "    dd = np.ones(len(xi), dtype='float')\n",
      "    for i in np.arange(len(xi)):\n",
      "        dist = np.sqrt((x-xi[i])**2 + (y-yi[i])**2)\n",
      "        ind[i] = dist.argmin()\n",
      "        dd[i] = dist[ind[i]]\n",
      "    return ind, dd\n",
      "\n",
      "\n",
      "def find_ij(x, y, d, xi, yi):\n",
      "    \"\"\"Find non-NaN cell d[j, i] that are closest to points (xi, yi).\"\"\"\n",
      "    index = np.where(~np.isnan(d.flatten()))[0]\n",
      "    ind, dd = nearxy(x.flatten()[index], y.flatten()[index], xi, yi)\n",
      "    j, i = ind2ij(x, index[ind])\n",
      "    return i, j, dd\n",
      "\n",
      "\n",
      "def find_timevar(cube):\n",
      "    \"\"\"Return the time variable from Iris. This is a workaround for Iris having\n",
      "    problems with FMRC aggregations, which produce two time coordinates.\"\"\"\n",
      "    try:\n",
      "        cube.coord(axis='T').rename('time')\n",
      "    except:\n",
      "        pass\n",
      "    timevar = cube.coord('time')\n",
      "    return timevar\n",
      "\n",
      "\n",
      "def ind2ij(a, index):\n",
      "    \"\"\"Returns a[j, i] for a.ravel()[index]. \"\"\"\n",
      "    n, m = a.shape\n",
      "    j = np.ceil(index//m)\n",
      "    i = np.remainder(index, m)\n",
      "    return i, j"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dateRange(start_date='1900-01-01', stop_date='2100-01-01',\n",
      "              constraint='overlaps'):\n",
      "    \"\"\"Hopefully something like this will be implemented in fes soon.\"\"\"\n",
      "    startname = 'apiso:TempExtent_begin'\n",
      "    endname = 'apiso:TempExtent_end'\n",
      "    if constraint == 'overlaps':\n",
      "        start = fes.PropertyIsLessThanOrEqualTo(propertyname=startname,\n",
      "                                                literal=stop_date)\n",
      "        stop = fes.PropertyIsGreaterThanOrEqualTo(propertyname=endname,\n",
      "                                                  literal=start_date)\n",
      "    elif constraint == 'within':\n",
      "        start = fes.PropertyIsGreaterThanOrEqualTo(propertyname=startname,\n",
      "                                                   literal=start_date)\n",
      "        stop = fes.PropertyIsLessThanOrEqualTo(propertyname=endname,\n",
      "                                               literal=stop_date)\n",
      "    return start, stop\n",
      "\n",
      "\n",
      "\n",
      "def service_urls(records,\n",
      "                 service='urn:x-esri:specification:ServiceType:odp:url'):\n",
      "    \"\"\"Extract service_urls of a specific type (DAP, SOS) from records andr\n",
      "    eturn the endpoint for a specified service type.\"\"\"\n",
      "    urls = []\n",
      "    for key, rec in records.items():\n",
      "        # Create a generator object, and iterate through it until the match is\n",
      "        # found if not found, gets the default value (here \"None\").\n",
      "        url = next((d['url'] for d in rec.references if\n",
      "                    d['scheme'] == service), None)\n",
      "        if url is not None:\n",
      "            urls.append(url)\n",
      "    return urls\n",
      "\n",
      "\n",
      "def get_Coops_longName(sta):\n",
      "    \"\"\"Get longName for specific station from COOPS SOS using Describe\n",
      "    Sensor request.\"\"\"\n",
      "    url = ('http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&'\n",
      "           'request=DescribeSensor&version=1.0.0&outputFormat=text/xml;'\n",
      "           'subtype=\"sensorML/1.0.1\"&'\n",
=======
      "# Constants and definitions.\n",
      "# Now we need to specify all the names we know for water level, names that\n",
      "# will get used in the CSW search, and also to find data in the datasets that\n",
      "# are returned.  This is ugly and fragile.  There hopefully will be a better\n",
      "# way in the future...\n",
      "\n",
      "name_list = ['water_surface_height_above_reference_datum',\n",
      "             'sea_surface_height_above_geoid',\n",
      "             'sea_surface_elevation',\n",
      "             'sea_surface_height_above_reference_ellipsoid',\n",
      "             'sea_surface_height_above_sea_level',\n",
      "             'sea_surface_height',\n",
      "             'water level']\n",
      "\n",
      "sos_name = 'water_surface_height_above_reference_datum'\n",
      "\n",
      "\n",
      "def get_Coops_longName(sta):\n",
      "    \"\"\"Get longName for specific station from COOPS SOS using DescribeSensor\n",
      "    request.\"\"\"\n",
      "    url = ('http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS?service=SOS&'\n",
      "           'request=DescribeSensor&version=1.0.0&'\n",
      "           'outputFormat=text/xml;subtype=\"sensorML/1.0.1\"&'\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "           'procedure=urn:ioos:station:NOAA.NOS.CO-OPS:%s') % sta\n",
      "    tree = etree.parse(urlopen(url))\n",
      "    root = tree.getroot()\n",
      "    path = \"//sml:identifier[@name='longName']/sml:Term/sml:value/text()\"\n",
<<<<<<< HEAD
      "    namespaces = {'sml':\n",
      "                  \"http://www.opengis.net/sensorML/1.0.1\"}\n",
=======
      "    namespaces = dict(sml=\"http://www.opengis.net/sensorML/1.0.1\")\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "    longName = root.xpath(path, namespaces=namespaces)\n",
      "    return longName\n",
      "\n",
      "\n",
      "def coops2df(collector, coops_id, sos_name):\n",
      "    \"\"\"Request CSV response from SOS and convert to Pandas DataFrames.\"\"\"\n",
      "    collector.features = [coops_id]\n",
      "    collector.variables = [sos_name]\n",
      "    response = collector.raw(responseFormat=\"text/csv\")\n",
<<<<<<< HEAD
      "    data_df = pd.read_csv(BytesIO(response.encode('utf-8')), parse_dates=True,\n",
      "                          index_col='date_time')\n",
      "    data_df['Observed Data'] = data_df['%s (m)' % sos_name]\n",
=======
      "    data_df = read_csv(BytesIO(response.encode('utf-8')),\n",
      "                       parse_dates=True,\n",
      "                       index_col='date_time')\n",
      "    col = 'water_surface_height_above_reference_datum (m)'\n",
      "    #data_df['Observed Data'] = data_df[col] - data_df['vertical_position (m)']\n",
      "    data_df['Observed Data'] = data_df[col]\n",
      "\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "    a = get_Coops_longName(coops_id)\n",
      "    if len(a) == 0:\n",
      "        long_name = coops_id\n",
      "    else:\n",
      "        long_name = a[0]\n",
<<<<<<< HEAD
=======
      "\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "    data_df.name = long_name\n",
      "    return data_df\n",
      "\n",
      "\n",
      "def mod_df(arr, timevar, istart, istop, mod_name, ts):\n",
      "    \"\"\"Return time series (DataFrame) from model interpolated onto uniform\n",
      "    time base.\"\"\"\n",
      "    t = timevar.points[istart:istop]\n",
      "    jd = timevar.units.num2date(t)\n",
      "\n",
<<<<<<< HEAD
      "    # Eliminate any data that is closer together than 10 seconds\n",
      "    # this was required to handle issues with CO-OPS aggregations, I think\n",
      "    # because they use floating point time in hours, which is not very\n",
      "    # accurate, so the FMRC aggregation is aggregating points that actually\n",
      "    # occur at the same time.\n",
=======
      "    # Eliminate any data that is closer together than 10 seconds this was\n",
      "    # required to handle issues with CO-OPS aggregations, I think because they\n",
      "    # use floating point time in hours, which is not very accurate, so the\n",
      "    # FMRC aggregation is aggregating points that actually occur at the same\n",
      "    # time.\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "    dt = np.diff(jd)\n",
      "    s = np.array([ele.seconds for ele in dt])\n",
      "    ind = np.where(s > 10)[0]\n",
      "    arr = arr[ind+1]\n",
      "    jd = jd[ind+1]\n",
      "\n",
<<<<<<< HEAD
      "    b = pd.DataFrame(arr, index=jd, columns=[mod_name])\n",
=======
      "    b = DataFrame(arr, index=jd, columns=[mod_name])\n",
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
      "    # Eliminate any data with NaN.\n",
      "    b = b[np.isfinite(b[mod_name])]\n",
      "    # Interpolate onto uniform time base, fill gaps up to:\n",
      "    # (10 values @ 6 min = 1 hour).\n",
<<<<<<< HEAD
      "    c = pd.concat([b, ts], axis=1).interpolate(limit=10) # FIXME!\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def name_in_list(cube):\n",
      "    \"\"\"Construct an Iris contraint to load only cubes that match the std_name_list.\"\"\"\n",
      "    return cube.standard_name in std_name_list\n",
      "\n",
      "constraint = iris.Constraint(cube_func=name_in_list)\n",
      "\n",
      "# FIXME: This is not working, need some re-factoring first.\n",
      "def model_obs_df(url, obs_df, jd_start, jd_stop, obs_lon, obs_lat, ts):\n",
      "    \"\"\"TODO: Consolidate the DataFrames!\"\"\"\n",
      "    a = iris.load_cube(url, constraint)\n",
      "    # Take first 20 chars for model name.\n",
      "    mod_name = a.attributes['title'][0:20]\n",
      "    r = a.shape\n",
      "    timevar = find_timevar(a)\n",
      "    lat = a.coord(axis='Y').points\n",
      "    lon = a.coord(axis='X').points\n",
      "    #jd = timevar.units.num2date(timevar.points)\n",
      "    start = timevar.units.date2num(jd_start)\n",
      "    stop = timevar.units.date2num(jd_stop)\n",
      "    istart = timevar.nearest_neighbour_index(start)\n",
      "    istop = timevar.nearest_neighbour_index(stop)\n",
      "\n",
      "    # Only proceed if we have data in the range requested.\n",
      "    if istart != istop:\n",
      "        print(url)\n",
      "        nsta = len(obs_lon)\n",
      "        if len(r) == 3:\n",
      "            d = a[0, ...].data\n",
      "            # Find the closest non-land point from a structured grid model.\n",
      "            j, i, dd = find_ij(lon, lat, d, obs_lon, obs_lat)\n",
      "            for n in range(nsta):\n",
      "                # Only use if model cell is within 0.1 degree of req, loc.\n",
      "                if dd[n] < 0.1:\n",
      "                    arr = a[istart:istop, j[n], i[n]].data\n",
      "                    c = mod_df(arr, timevar, istart, istop, mod_name, ts)\n",
      "                    name = obs_df[n].name\n",
      "                    obs_df[n] = pd.concat([obs_df[n], c], axis=1)\n",
      "                    obs_df[n].name = name\n",
      "        elif len(r) == 2:\n",
      "            # Find the closest point from an unstructured grid model.\n",
      "            index, dd = nearxy(lon.flatten(), lat.flatten(),\n",
      "                               obs_lon, obs_lat)\n",
      "            for n in range(nsta):\n",
      "                # Only use if model cell is within 0.1 degree of req. loc.\n",
      "                if dd[n] < 0.1:\n",
      "                    arr = a[istart:istop, index[n]].data\n",
      "                    c = mod_df(arr, timevar, istart, istop, mod_name, ts)\n",
      "                    name = obs_df[n].name\n",
      "                    obs_df[n] = pd.concat([obs_df[n], c], axis=1)\n",
      "                    obs_df[n].name = name\n",
      "        return obs_df"
=======
      "    c = concat([b, ts], axis=1).interpolate(limit=10)\n",
      "    return c\n",
      "\n",
      "\n",
      "def dateRange(start_date='1900-01-01', stop_date='2100-01-01',\n",
      "              constraint='overlaps'):\n",
      "    \"\"\"Hopefully something like this will be implemented in fes soon.\"\"\"\n",
      "    if constraint == 'overlaps':\n",
      "        begin = 'apiso:TempExtent_begin'\n",
      "        end = 'apiso:TempExtent_end'\n",
      "        start = fes.PropertyIsLessThanOrEqualTo(propertyname=begin,\n",
      "                                                literal=stop_date)\n",
      "        stop = fes.PropertyIsGreaterThanOrEqualTo(propertyname=end,\n",
      "                                                  literal=start_date)\n",
      "    elif constraint == 'within':\n",
      "        start = fes.PropertyIsGreaterThanOrEqualTo(propertyname=begin,\n",
      "                                                   literal=start_date)\n",
      "        stop = fes.PropertyIsLessThanOrEqualTo(propertyname=end,\n",
      "                                               literal=stop_date)\n",
      "    return start, stop\n",
      "\n",
      "\n",
      "def service_urls(records, service='odp:url'):\n",
      "    \"\"\"Extract service_urls of a specific type (DAP, SOS) from records.\"\"\"\n",
      "    service_string = 'urn:x-esri:specification:ServiceType:' + service\n",
      "    urls = []\n",
      "    for key, rec in records.iteritems():\n",
      "        # Create a generator object, and iterate through it until the match is\n",
      "        # found if not found, gets the default value (here \"none\").\n",
      "        url = next((d['url'] for d in rec.references if\n",
      "                    d['scheme'] == service_string), None)\n",
      "        if url is not None:\n",
      "            urls.append(url)\n",
      "    return urls\n",
      "\n",
      "\n",
      "def nearxy(x, y, xi, yi):\n",
      "    \"\"\"Find the indices x[i] of arrays (x,y) closest to the points (xi,yi).\"\"\"\n",
      "    ind = np.ones(len(xi), dtype=int)\n",
      "    dd = np.ones(len(xi), dtype='float')\n",
      "    for i in np.arange(len(xi)):\n",
      "        dist = np.sqrt((x-xi[i])**2 + (y-yi[i])**2)\n",
      "        ind[i] = dist.argmin()\n",
      "        dd[i] = dist[ind[i]]\n",
      "    return ind, dd\n",
      "\n",
      "\n",
      "def find_ij(x, y, d, xi, yi):\n",
      "    \"\"\"Find non-NaN cell d[j, i] that are closest to points (xi, yi).\"\"\"\n",
      "    index = np.where(~np.isnan(d.flatten()))[0]\n",
      "    ind, dd = nearxy(x.flatten()[index],\n",
      "                     y.flatten()[index], xi, yi)\n",
      "    j, i = ind2ij(x, index[ind])\n",
      "    return i, j, dd\n",
      "\n",
      "\n",
      "def find_timevar(cube):\n",
      "    \"\"\"Return the time variable from Iris.  This is a workaround for\n",
      "    Iris having problems with FMRC aggregations, which produce two time\n",
      "    coordinates.\"\"\"\n",
      "    try:\n",
      "        cube.coord(axis='T').rename('time')\n",
      "    except:\n",
      "        pass\n",
      "    timevar = cube.coord('time')\n",
      "    return timevar\n",
      "\n",
      "\n",
      "def ind2ij(a, index):\n",
      "    \"\"\"Returns a[j,i] for a.ravel()[index].\"\"\"\n",
      "    n, m = a.shape\n",
      "    j = np.ceil(index // m)\n",
      "    i = np.remainder(index, m)\n",
      "    return i, j"
>>>>>>> afa17fb799acb5e2809bff9436e96bafe3cdc3f9
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}